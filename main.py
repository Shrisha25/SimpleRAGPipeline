# -*- coding: utf-8 -*-
"""Quanti_RAG.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RgayhlAZJKb6wNgfXUssZuITzsLvk8tq
"""

#import necessary pacakges
import torch
from transformers import pipeline, AutoTokenizer
import fitz
from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.llms import HuggingFacePipeline
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.runnables import RunnablePassthrough

"""# Basic Question Answering Pipeline"""

def extract_text_from_pdf(pdf_path):
    """
    Extract text from a PDF document.

    Args:
        pdf_path (str): Path to the PDF file.

    Returns:
        str: Extracted text from the PDF.
    """
    text = ''
    with fitz.open(pdf_path) as doc:
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text += page.get_text()
    return text

# Specify the path to the PDF file on your local system
local_pdf_path = "/content/chapter_text.pdf"

# Load the model and tokenizer
model_name = "distilbert-base-uncased-distilled-squad" # Provide the model name or path here
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = pipeline("question-answering", model=model_name, tokenizer=tokenizer)

# Extract text from the PDF document
pdf_text = extract_text_from_pdf(local_pdf_path)

# Define the question
question = "What is a cell membrane?"

# Perform question answering
result = model(question=question, context=pdf_text)

# Print the answer
print("Answer to the question:", result['answer'])

"""#  Advanced Question Answering Pipeline with Text Preprocessing"""

# Load the model and tokenizer
model_name = "distilbert-base-uncased-distilled-squad"#"monologg/biobert_v1.1_pubmed"#"bert-base-uncased"#"distilbert-base-uncased-distilled-squad"
tokenizer = AutoTokenizer.from_pretrained(model_name)
qa_model = pipeline("question-answering", model=model_name, tokenizer=tokenizer)

# Load a sentence-transformers model to generate embeddings
embedding_model = SentenceTransformer('paraphrase-distilroberta-base-v2')

def extract_text_from_pdf(pdf_path):
    text = ''
    with fitz.open(pdf_path) as doc:
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text += page.get_text()
    return text

def preprocess_text(text):
    # Remove special characters and extra whitespaces
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\s+', ' ', text)
    # Convert text to lowercase
    text = text.lower()
    return text

def chunk_text(text, chunk_size=1000):
    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    return chunks

def generate_embeddings(text_chunks):
    embeddings = []
    for chunk in text_chunks:
        chunk_embedding = embedding_model.encode(chunk)
        embeddings.append(chunk_embedding)
    return embeddings

def answer_questions(text_chunks, questions):
    answers = {}
    for question in questions:
        question_answers = []
        for chunk in text_chunks:
            result = qa_model(question=question, context=chunk)
            question_answers.append(result['answer'])
        answers[question] = " ".join(question_answers)
    return answers

local_pdf_path = "/content/chapter_text.pdf"
pdf_text = extract_text_from_pdf(local_pdf_path)
pdf_text = preprocess_text(pdf_text)

# Chunk the text into smaller sections
text_chunks = chunk_text(pdf_text)

# Generate embeddings for text chunks
embeddings = generate_embeddings(text_chunks)

# Define the questions
questions = ["What is a cell?"]

# Perform question answering with each chunk as context
answers = answer_questions(text_chunks, questions)

# Print the answers
for question, answer in answers.items():
    print(f"Answer to the question '{question}': {answer}")

"""# Text-Generation using Langchain"""

class Document:
    def __init__(self, page_content):
        self.page_content = page_content
        self.metadata = {}  # Provide an empty dictionary as metadata



# Function to extract text from a PDF document
def extract_text_from_pdf(pdf_path):
    text = ''
    with fitz.open(pdf_path) as doc:
        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            text += page.get_text()
    return text

# Load the PDF document
local_pdf_path = "/content/chapter_text.pdf"
pdf_text = extract_text_from_pdf(local_pdf_path)

# Split the document into chunks
splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=30)
chunked_texts = splitter.split_text(pdf_text)

# Create Document objects from text chunks
documents = [Document(page_content=chunk) for chunk in chunked_texts]

# Initialize FAISS index and retriever
db = FAISS.from_documents(documents, HuggingFaceEmbeddings(model_name="BAAI/bge-base-en-v1.5"))
retriever = db.as_retriever(search_type="similarity", search_kwargs={"k": 4})

# Load the Hugging Face model and tokenizer for language generation
# model_name = "HuggingFaceH4/zephyr-7b-beta"
model_name = "bert-base-uncased"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Construct the language model pipeline for text generation
text_generation_pipeline = pipeline(
    model=model,                  # The pretrained language model to use for text generation
    tokenizer=tokenizer,          # The tokenizer corresponding to the model
    task="text-generation",       # The task to perform using the pipeline (text generation)
    temperature=0.2,              # Controls the randomness of the generated text; lower values lead to more deterministic outputs
    do_sample=True,               # Whether to use sampling when generating text (allows for more diverse outputs)
    repetition_penalty=1.1,       # Penalty to apply to repeated tokens in the generated text; higher values discourage repetition
    return_full_text=True,        # Whether to return the full text or just the generated sequence
    max_new_tokens=400,           # Maximum number of tokens to generate; controls the length of the generated text
)


llm = HuggingFacePipeline(pipeline=text_generation_pipeline)

# Define a prompt template for the language model pipeline
prompt_template = """
Answer the question based on your knowledge. Use the following context to help:

{context}

</s>

{question}
</s>
"""

# Create a PromptTemplate object
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template,
)

# Combine the prompt and language model pipeline
llm_chain = prompt | llm | StrOutputParser()

from langchain_core.runnables import RunnablePassthrough

retriever = db.as_retriever()

rag_chain = {"context": retriever, "question": RunnablePassthrough()} | llm_chain

# Define the question
question = "What is cell?"
llm_chain.invoke({"context": "", "question": question})



